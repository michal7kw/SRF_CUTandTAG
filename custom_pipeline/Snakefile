# Snakefile for cut&tag analysis

import os
from os.path import join
from snakemake.utils import min_version
import re

# Set minimum snakemake version
min_version("6.0")

# Load configuration from config.yaml file
configfile: "config.yaml"

# Define resource configurations
resource_config = {
    "fastqc": {"mem_mb": 8000, "ntasks": 4, "runtime": 120},      # 2 hours
    "trim_reads": {"mem_mb": 16000, "ntasks": 16, "runtime": 360}, # 6 hours
    "align": {"mem_mb": 32000, "ntasks": 32, "runtime": 720},     # 12 hours
    "call_peaks": {"mem_mb": 16000, "ntasks": 16, "runtime": 480}, # 8 hours
    "fragment_sizes": {"mem_mb": 8000, "ntasks": 8, "runtime": 60} # 1 hour
}

# Define local rules
localrules: all, multiqc

# raw fastq files for each experiment
EXOGENOUS = "DATA/EXOGENOUS"
ENDOGENOUS = "DATA/ENDOGENOUS"

# main output directory where all results will be stored
OUTPUT = "results"

# Get sample names from input directories
EXOGENOUS_SAMPLES = [f.split("_R1")[0] for f in os.listdir(EXOGENOUS) if f.endswith("R1_001.fastq.gz")]
ENDOGENOUS_SAMPLES = [f.split("_R1")[0] for f in os.listdir(ENDOGENOUS) if f.endswith("R1_001.fastq.gz")]

# Make sure IgM is in ENDOGENOUS_SAMPLES if it's not already there
if "IgM" not in ENDOGENOUS_SAMPLES and "IgM_R1_001.fastq.gz" in os.listdir(ENDOGENOUS):
    ENDOGENOUS_SAMPLES.append("IgM")

# Define ALL_SAMPLES
ALL_SAMPLES = list(set(EXOGENOUS_SAMPLES + ENDOGENOUS_SAMPLES))
print(f"Running in full mode with {len(ALL_SAMPLES)} samples")

# Function to get input directory for a sample
def get_input_dir(sample):
    # For IgM (control), always use ENDOGENOUS directory
    if sample == "IgM":
        return ENDOGENOUS
    # For other samples, check their respective directories
    elif sample in EXOGENOUS_SAMPLES:
        return EXOGENOUS
    elif sample in ENDOGENOUS_SAMPLES:
        return ENDOGENOUS
    else:
        raise ValueError(f"Unknown sample: {sample}")

# Function to get experiment type for a sample
def get_experiment(sample):
    if sample in EXOGENOUS_SAMPLES:
        return "EXOGENOUS"
    elif sample in ENDOGENOUS_SAMPLES:
        return "ENDOGENOUS"
    elif sample == "IgM":  # Control sample can be used for both
        return "CONTROL"
    else:
        raise ValueError(f"Unknown sample: {sample}")

# specifies all output files to be generated
rule all:
    input:
        # Basic processing (always included)
        expand(join(OUTPUT, "fastqc", "{sample}_R1_001_fastqc.html"), sample=ALL_SAMPLES),
        expand(join(OUTPUT, "fastqc", "{sample}_R2_001_fastqc.html"), sample=ALL_SAMPLES),
        expand(join(OUTPUT, "trimmed", "{sample}_R1_001_val_1.fq.gz"), sample=ALL_SAMPLES),
        expand(join(OUTPUT, "trimmed", "{sample}_R2_001_val_2.fq.gz"), sample=ALL_SAMPLES),
        expand(join(OUTPUT, "aligned", "{sample}.bam"), sample=ALL_SAMPLES),
        expand(join(OUTPUT, "aligned", "{sample}.bam.bai"), sample=ALL_SAMPLES),
        expand(join(OUTPUT, "peaks", "{sample}_peaks.narrowPeak"), sample=ALL_SAMPLES)

# Rule to run FastQC on raw reads
# performs quality control checks on the raw sequencing data
rule fastqc:
    input:
        r1 = lambda wildcards: os.path.join(get_input_dir(wildcards.sample), 
                                          f"{wildcards.sample}_R1_001.fastq.gz"),
        r2 = lambda wildcards: os.path.join(get_input_dir(wildcards.sample), 
                                          f"{wildcards.sample}_R2_001.fastq.gz")
    output:
        html_r1 = join(OUTPUT, "fastqc", "{sample}_R1_001_fastqc.html"),
        html_r2 = join(OUTPUT, "fastqc", "{sample}_R2_001_fastqc.html"),
        zip_r1 = join(OUTPUT, "fastqc", "{sample}_R1_001_fastqc.zip"),
        zip_r2 = join(OUTPUT, "fastqc", "{sample}_R2_001_fastqc.zip")
    log:
        join(OUTPUT, "logs", "fastqc", "{sample}.log")
    threads: 2
    resources:
        skip=lambda w, attempt: resources.get("skip_fastqc", 0)
    envmodules:
        "fastqc/0.11.9"
    shell:
        """
        if [ "{resources.skip}" = "1" ]; then
            touch {output.html_r1} {output.html_r2} {output.zip_r1} {output.zip_r2}
        else
            fastqc {input.r1} {input.r2} -o $(dirname {output.html_r1}) -t {threads} 2> {log}
        fi
        """

# Rule to trim reads using Trim Galore
rule trim_reads:
    input:
        r1 = lambda wildcards: os.path.join(get_input_dir(wildcards.sample), 
                                          f"{wildcards.sample}_R1_001.fastq.gz"),
        r2 = lambda wildcards: os.path.join(get_input_dir(wildcards.sample), 
                                          f"{wildcards.sample}_R2_001.fastq.gz")
    output:
        r1 = join(OUTPUT, "trimmed", "{sample}_R1_001_val_1.fq.gz"),
        r2 = join(OUTPUT, "trimmed", "{sample}_R2_001_val_2.fq.gz")
    log:
        join(OUTPUT, "logs", "trim_galore", "{sample}.log")
    threads: 16
    resources:
        mem_mb=32000,
        time="6:00:00",
        skip=lambda w, attempt: resources.get("skip_trim", 0)
    envmodules:
        "trimgalore/0.6.6",
        "fastqc/0.11.9"
    shell:
        """
        if [ "{resources.skip}" = "1" ]; then
            touch {output.r1} {output.r2}
        else
            mkdir -p {OUTPUT}/trimmed
            trim_galore --paired --gzip --fastqc --cores {threads} \
                --output_dir {OUTPUT}/trimmed \
                {input.r1} {input.r2} &> {log}
        fi
        """

# Rule to align trimmed reads
rule align:
    input:
        r1 = rules.trim_reads.output.r1,
        r2 = rules.trim_reads.output.r2
    output:
        bam = temp(join(OUTPUT, "aligned", "{sample}.unsorted.bam")),
        sorted_bam = join(OUTPUT, "aligned", "{sample}.bam"),
        bai = join(OUTPUT, "aligned", "{sample}.bam.bai")
    log:
        join(OUTPUT, "logs", "bowtie2", "{sample}.log")
    threads: 32
    resources:
        mem_mb=96000,
        time="8:00:00",
        skip=lambda w, attempt: resources.get("skip_align", 0)
    params:
        max_fragment = config["bowtie2_params"]["max_fragment_length"],
        sort_memory = "2G",
        tmp_dir = "/beegfs/scratch/ric.broccoli/kubacki.michal/SRF_CUTandTAG/custom_pipeline/tmp"
    envmodules:
        "bowtie2/2.4.2",
        "samtools/1.13"
    shell:
        """
        if [ "{resources.skip}" = "1" ]; then
            touch {output.bam}
            touch {output.sorted_bam}
            touch {output.bai}
        else
            # Create temporary directory for this job
            TEMP_DIR=$(mktemp -d -p {params.tmp_dir})
            
            # Align reads with optimized parameters
            bowtie2 \
                -p {threads} \
                -x {config[genome_index]} \
                -1 {input.r1} -2 {input.r2} \
                --local --very-sensitive-local \
                --no-mixed --no-discordant \
                --maxins {params.max_fragment} \
                --mm \
                2> {log} | \
            samtools view -@ {threads} -bS -q 30 - > {output.bam}

            # Sort BAM file using multiple threads and temporary directory
            samtools sort \
                -@ {threads} \
                -m {params.sort_memory} \
                -T $TEMP_DIR/{wildcards.sample} \
                -o {output.sorted_bam} \
                {output.bam}

            # Index BAM file
            samtools index -@ {threads} {output.sorted_bam}
            
            # Cleanup
            rm -rf $TEMP_DIR
        fi
        """

# Rule to call peaks using MACS2
rule call_peaks:
    input:
        treatment = join(OUTPUT, "aligned", "{sample}.bam"),
        treatment_index = join(OUTPUT, "aligned", "{sample}.bam.bai"),
        control = lambda wildcards: [] if wildcards.sample == "IgM" else join(OUTPUT, "aligned", "IgM.bam"),
        control_index = lambda wildcards: [] if wildcards.sample == "IgM" else join(OUTPUT, "aligned", "IgM.bam.bai")
    output:
        peaks = join(OUTPUT, "peaks", "{sample}_peaks.narrowPeak")
    log:
        join(OUTPUT, "logs", "macs2", "{sample}.log")
    threads: 16
    resources:
        mem_mb=32000,
        time="4:00:00"
    params:
        genome_size = config['effective_genome_size'],
        format = "BAMPE",
        qvalue = "0.05",
        outdir = lambda wildcards, output: os.path.dirname(output.peaks),
        control_param = lambda wildcards, input: "" if wildcards.sample == "IgM" else f"-c {input.control}"
    envmodules:
        "macs2/2.2.7.1"
    shell:
        """
        mkdir -p {params.outdir}
        macs2 callpeak \
            -t {input.treatment} \
            {params.control_param} \
            -f {params.format} \
            -g {params.genome_size} \
            -n {wildcards.sample} \
            --outdir {params.outdir} \
            -q {params.qvalue} \
            --nomodel \
            --keep-dup all \
            --call-summits \
            2> {log}
        """

# Rule to run MultiQC to aggregate QC reports
# compiles all quality control metrics into a single report
rule multiqc:
    # takes all fastqc and aligned bam files from previous rules
    input:
        expand(join(OUTPUT, "fastqc", "{sample}_R1_001_fastqc.html"), sample=ALL_SAMPLES),
        expand(join(OUTPUT, "fastqc", "{sample}_R2_001_fastqc.html"), sample=ALL_SAMPLES),
        expand(join(OUTPUT, "aligned", "{sample}.bam"), sample=ALL_SAMPLES)
    # outputs multiqc report
    output:
        report = join(OUTPUT, "multiqc", "multiqc_report.html")
    log:
        join(OUTPUT, "logs", "multiqc", "multiqc.log")
    shell:
        "multiqc {OUTPUT} -o {OUTPUT}/multiqc &> {log}"

# Add localrules for non-computational steps
localrules: all, multiqc

# Add rule to analyze fragment sizes
rule fragment_sizes:
    input:
        bam = join(OUTPUT, "aligned", "{sample}.bam"),
        bai = join(OUTPUT, "aligned", "{sample}.bam.bai")
    output:
        pdf = join(OUTPUT, "fragment_sizes", "{sample}_fragment_sizes.pdf"),
        txt = join(OUTPUT, "fragment_sizes", "{sample}_fragment_sizes.txt")
    log:
        join(OUTPUT, "logs", "fragment_sizes", "{sample}.log")
    threads: 16
    resources:
        mem_mb=16000,
        time="1:00:00"
    envmodules:
        "samtools/1.13",
        "R/4.1.0"
    shell:
        """
        # Calculate fragment sizes
        samtools view {input.bam} | \
        awk '{{if ($9 > 0) print $9}}' > {output.txt}

        # Plot fragment size distribution
        Rscript -e '
        pdf("{output.pdf}")
        data <- read.table("{output.txt}")
        hist(data$V1, breaks=100, main="{wildcards.sample} Fragment Size Distribution",
             xlab="Fragment Size", ylab="Count")
        dev.off()
        ' 2> {log}
        """

# Rule to perform extended analysis
rule extended_analysis:
    input:
        peaks = expand(join(OUTPUT, "peaks", "{sample}_peaks.narrowPeak"), sample=ALL_SAMPLES),
        bams = expand(join(OUTPUT, "aligned", "{sample}.bam"), sample=ALL_SAMPLES)
    output:
        report = join(OUTPUT, "extended_analysis", "analysis_report.html")
    log:
        join(OUTPUT, "logs", "extended_analysis", "extended_analysis.log")
    script:
        "scripts/extended_analysis.R"

rule cross_correlation:
    input:
        bam = rules.align.output.sorted_bam
    output:
        plot = join(OUTPUT, "qc", "{sample}.crosscorr.pdf"),
        metrics = join(OUTPUT, "qc", "{sample}.crosscorr.txt")
    script:
        "scripts/cross_correlation.R"

rule calculate_frip:
    input:
        bam = rules.align.output.sorted_bam,
        peaks = rules.call_peaks.output.peaks
    output:
        frip = join(OUTPUT, "qc", "{sample}.frip_score.txt")
    script:
        "scripts/calculate_frip.R"

rule generate_md5:
    input:
        fastq = lambda wildcards: join(get_input_dir(wildcards.sample), 
                                     f"{wildcards.sample}_R{wildcards.read}_001.fastq.gz")
    output:
        md5 = join(OUTPUT, "md5", "{sample}_R{read}_001.md5")
    shell:
        "md5sum {input.fastq} > {output.md5}"

rule generate_report:
    input:
        qc_metrics = expand(join(OUTPUT, "qc", "{sample}.{metric}.txt"),
                          sample=ALL_SAMPLES,
                          metric=["frip_score", "tss_enrichment"]),
        peak_metrics = join(OUTPUT, "peak_analysis", "peak_reproducibility.txt")
    output:
        report = join(OUTPUT, "report", "pipeline_report.html")
    script:
        "scripts/generate_report.R"

# Add rule for bigwig generation
rule make_bigwig:
    input:
        bam = join(OUTPUT, "aligned", "{sample}.bam"),
        bai = join(OUTPUT, "aligned", "{sample}.bam.bai")
    output:
        bw = join(OUTPUT, "bigwig", "{sample}.bw")
    params:
        bin_size = config["visualization"]["bigwig"]["bin_size"],
        smooth_length = config["visualization"]["bigwig"]["smooth_length"]
    log:
        join(OUTPUT, "logs", "bigwig", "{sample}.log")
    threads: 4
    envmodules:
        "deeptools/3.5.1"
    shell:
        """
        bamCoverage --bam {input.bam} \
            --outFileName {output.bw} \
            --binSize {params.bin_size} \
            --smoothLength {params.smooth_length} \
            --normalizeUsing RPKM \
            --numberOfProcessors {threads} \
            2> {log}
        """

# Add rule for TSS enrichment
rule tss_enrichment:
    input:
        bam = join(OUTPUT, "aligned", "{sample}.bam"),
        bai = join(OUTPUT, "aligned", "{sample}.bam.bai")
    output:
        score = join(OUTPUT, "qc", "{sample}.tss_enrichment.txt")
    params:
        gtf = config["tss_analysis"]["genome_gtf"],
        upstream = config["tss_analysis"]["upstream"],
        downstream = config["tss_analysis"]["downstream"]
    log:
        join(OUTPUT, "logs", "tss_enrichment", "{sample}.log")
    envmodules:
        "R/4.1.0"
    script:
        "scripts/calculate_tss_enrichment.R"

# Add rule for metagene plot
rule metagene_plot:
    input:
        bw = join(OUTPUT, "bigwig", "{sample}.bw"),
        peaks = join(OUTPUT, "peaks", "{sample}_peaks.narrowPeak")
    output:
        plot = join(OUTPUT, "plots", "{sample}_metagene.pdf")
    params:
        upstream = config["tss_analysis"]["upstream"],
        downstream = config["tss_analysis"]["downstream"]
    log:
        join(OUTPUT, "logs", "plots", "{sample}_metagene.log")
    envmodules:
        "R/4.1.0"
    script:
        "scripts/plot_metagene.R"

# Add rule for heatmap
rule heatmap_plot:
    input:
        bw = join(OUTPUT, "bigwig", "{sample}.bw"),
        peaks = join(OUTPUT, "peaks", "{sample}_peaks.narrowPeak")
    output:
        plot = join(OUTPUT, "plots", "{sample}_heatmap.pdf")
    params:
        window_size = config["visualization"]["heatmap"]["window_size"],
        bin_size = config["visualization"]["heatmap"]["bin_size"]
    log:
        join(OUTPUT, "logs", "plots", "{sample}_heatmap.log")
    envmodules:
        "R/4.1.0"
    script:
        "scripts/plot_heatmap.R"

# Rule to analyze peak reproducibility
rule analyze_peak_reproducibility:
    input:
        peaks = expand(join(OUTPUT, "peaks", "{sample}_peaks.narrowPeak"), sample=ALL_SAMPLES)
    output:
        txt = join(OUTPUT, "peak_analysis", "peak_reproducibility.txt")
    log:
        join(OUTPUT, "logs", "peak_analysis", "reproducibility.log")
    threads: 4
    resources:
        mem_mb=8000,
        time="1:00:00"
    envmodules:
        "R/4.1.0"
    shell:
        """
        Rscript scripts/analyze_reproducibility.R \
            --peaks {input.peaks} \
            --output {output.txt} \
            2> {log}
        """

# Rule to annotate peaks
rule annotate_peaks:
    input:
        peaks = expand(join(OUTPUT, "peaks", "{sample}_peaks.narrowPeak"), sample=ALL_SAMPLES)
    output:
        txt = join(OUTPUT, "peak_analysis", "peak_annotation.txt")
    log:
        join(OUTPUT, "logs", "peak_analysis", "annotation.log")
    threads: 4
    resources:
        mem_mb=8000,
        time="1:00:00"
    envmodules:
        "R/4.1.0"
    shell:
        """
        Rscript scripts/annotate_peaks.R \
            --peaks {input.peaks} \
            --output {output.txt} \
            --genome {config[genome_gtf]} \
            2> {log}
        """

# rule analyze_peak_enrichment:
#     input:
#         dea = "DATA/DEA_NSC.csv",
#         peaks = expand("results/peaks/{sample}_peaks.narrowPeak", 
#                       sample=ALL_SAMPLES)
#     output:
#         enrichment_plots = "results/enrichment_analysis.pdf",
#         enrichment_signal = "results/enrichment_signal_ratio.csv",
#         enrichment_count = "results/enrichment_peak_count.csv",
#         enrichment_combined = "results/enrichment_combined_score.csv",
#         enrichment_stats = "results/enrichment_statistical.csv"
#     log:
#         "logs/enrichment/peak_enrichment.log"
#     conda:
#         "envs/analysis.yaml"
#     script:
#         "scripts/analyze_enrichment.py"

# Add wildcard constraints
wildcard_constraints:
    sample="|".join([re.escape(x) for x in (EXOGENOUS_SAMPLES + ENDOGENOUS_SAMPLES + ["IgM"])]),
    frr="[12]"  # Assuming frr can only be 1 or 2

# Add this function to the top section with the other helper functions
def get_samples_for_experiment(experiment):
    if experiment == "EXOGENOUS":
        return EXOGENOUS_SAMPLES
    elif experiment == "ENDOGENOUS":
        return ENDOGENOUS_SAMPLES
    elif experiment == "CONTROL":
        return ["IgM"]
    else:
        raise ValueError(f"Unknown experiment: {experiment}")

