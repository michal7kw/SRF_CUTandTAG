# Snakefile for cut&tag analysis

import os
from os.path import join
from snakemake.utils import min_version

# Set minimum snakemake version
min_version("6.0")

# Load configuration from config.yaml file
configfile: "config.yaml"

# Define resource configurations
resource_config = {
    "fastqc": {"mem_mb": 8000, "ntasks": 2, "runtime": 120},      # 2 hours
    "trim_reads": {"mem_mb": 16000, "ntasks": 8, "runtime": 360}, # 6 hours
    "align": {"mem_mb": 32000, "ntasks": 16, "runtime": 720},     # 12 hours
    "call_peaks": {"mem_mb": 16000, "ntasks": 8, "runtime": 480}, # 8 hours
    "fragment_sizes": {"mem_mb": 8000, "ntasks": 4, "runtime": 60} # 1 hour
}

# Define local rules
localrules: all, multiqc

# raw fastq files for each experiment
EXOGENOUS = "DATA/EXOGENOUS"
ENDOGENOUS = "DATA/ENDOGENOUS"

# main output directory where all results will be stored
OUTPUT = "results"

# Get sample names from input directories
EXOGENOUS_SAMPLES = [f.split("_R1")[0] for f in os.listdir(EXOGENOUS) if f.endswith("R1_001.fastq.gz")]
ENDOGENOUS_SAMPLES = [f.split("_R1")[0] for f in os.listdir(ENDOGENOUS) if f.endswith("R1_001.fastq.gz")]

# Define control samples
CONTROL_SAMPLES = {
    "EXOGENOUS": config["EXOGENOUS_control"],  # Should be "IgM"
    "ENDOGENOUS": config["ENDOGENOUS_control"]  # Should be "IgM"
}

# Function to get input directory for a sample
def get_input_dir(sample):
    # For IgM (control), always use ENDOGENOUS directory
    if sample == "IgM":
        return ENDOGENOUS
    # For other samples, check their respective directories
    elif sample in EXOGENOUS_SAMPLES:
        return EXOGENOUS
    elif sample in ENDOGENOUS_SAMPLES:
        return ENDOGENOUS
    else:
        raise ValueError(f"Sample {sample} not found in either EXOGENOUS or ENDOGENOUS directories")

# Function to get experiment type for a sample
def get_experiment(sample):
    if sample in EXOGENOUS_SAMPLES:
        return "EXOGENOUS"
    elif sample in ENDOGENOUS_SAMPLES:
        return "ENDOGENOUS"
    elif sample == "IgM":  # Control sample can be used for both
        return "CONTROL"
    else:
        raise ValueError(f"Unknown sample: {sample}")

# Override if a specific sample is specified for testing
if config.get("sample"):
    sample = config["sample"]
    # Check if sample exists (including IgM in ENDOGENOUS)
    if sample in ENDOGENOUS_SAMPLES or sample in EXOGENOUS_SAMPLES or (sample == "IgM" and "IgM_R1_001.fastq.gz" in os.listdir(ENDOGENOUS)):
        ALL_SAMPLES = [sample]
        print(f"Running in test mode with sample: {sample}")
        print(f"Sample found in: {get_input_dir(sample)}")
        if sample == "IgM":
            print("IgM is being used as control for both ENDOGENOUS and EXOGENOUS experiments")
    else:
        raise ValueError(f"Test sample {sample} not found in data directories")
else:
    ALL_SAMPLES = list(set(EXOGENOUS_SAMPLES + ENDOGENOUS_SAMPLES))  # Use set to avoid duplicates
    print(f"Running in full mode with {len(ALL_SAMPLES)} samples")

# specifies all output files to be generated
rule all:
    input:
        # Basic processing (always included)
        expand(join(OUTPUT, "fastqc", "{sample}_R1_001_fastqc.html"), sample=ALL_SAMPLES),
        expand(join(OUTPUT, "fastqc", "{sample}_R2_001_fastqc.html"), sample=ALL_SAMPLES),
        expand(join(OUTPUT, "trimmed", "{sample}_R1_001_val_1.fq.gz"), sample=ALL_SAMPLES),
        expand(join(OUTPUT, "trimmed", "{sample}_R2_001_val_2.fq.gz"), sample=ALL_SAMPLES),
        expand(join(OUTPUT, "aligned", "{sample}.bam"), sample=ALL_SAMPLES),
        expand(join(OUTPUT, "peaks", "{sample}_peaks.narrowPeak"), sample=ALL_SAMPLES),
        expand(join(OUTPUT, "qc", "{sample}.frip_score.txt"), sample=ALL_SAMPLES),
        expand(join(OUTPUT, "qc", "{sample}.tss_enrichment.txt"), sample=ALL_SAMPLES),
        expand(join(OUTPUT, "bigwig", "{sample}.bw"), sample=ALL_SAMPLES),
        expand(join(OUTPUT, "plots", "{sample}_heatmap.pdf"), sample=ALL_SAMPLES),
        expand(join(OUTPUT, "plots", "{sample}_metagene.pdf"), sample=ALL_SAMPLES),
        
        # Multi-sample analyses (only included when not in test mode)
        [] if config.get("sample") else [
            join(OUTPUT, "peak_analysis", "peak_reproducibility.txt"),
            join(OUTPUT, "peak_analysis", "peak_annotation.txt"),
            join(OUTPUT, "multiqc", "multiqc_report.html")
        ]

# Rule to run FastQC on raw reads
# performs quality control checks on the raw sequencing data
rule fastqc:
    # takes R1 and R2 files from input directory and runs
    input:
        r1 = lambda wildcards: os.path.join(get_input_dir(wildcards.sample), 
                                          f"{wildcards.sample}_R1_001.fastq.gz"),
        r2 = lambda wildcards: os.path.join(get_input_dir(wildcards.sample), 
                                          f"{wildcards.sample}_R2_001.fastq.gz")
    # outputs html and zip files with quality metrics
    output:
        html_r1 = join(OUTPUT, "fastqc", "{sample}_R1_001_fastqc.html"),
        html_r2 = join(OUTPUT, "fastqc", "{sample}_R2_001_fastqc.html"),
        zip_r1 = join(OUTPUT, "fastqc", "{sample}_R1_001_fastqc.zip"),
        zip_r2 = join(OUTPUT, "fastqc", "{sample}_R2_001_fastqc.zip")
    log:
        join(OUTPUT, "logs", "fastqc", "{sample}.log")
    threads: 2
    envmodules:
        "fastqc/0.11.9"
    shell:
        """
        fastqc {input.r1} {input.r2} -o $(dirname {output.html_r1}) -t {threads} 2> {log}
        """

# Rule to trim reads using Trim Galore
rule trim_reads:
    input:
        r1 = lambda wildcards: join(get_input_dir(wildcards.sample), f"{wildcards.sample}_R1_001.fastq.gz"),
        r2 = lambda wildcards: join(get_input_dir(wildcards.sample), f"{wildcards.sample}_R2_001.fastq.gz")
    output:
        r1 = join(OUTPUT, "trimmed", "{sample}_R1_001_val_1.fq.gz"),
        r2 = join(OUTPUT, "trimmed", "{sample}_R2_001_val_2.fq.gz")
    log:
        join(OUTPUT, "logs", "trim_galore", "{sample}.log")
    threads: 16
    resources:
        mem_mb=32000,
        time="6:00:00"
    envmodules:
        "trimgalore/0.6.6",
        "fastqc/0.11.9"
    shell:
        """
        mkdir -p {OUTPUT}/trimmed
        trim_galore --paired --gzip --fastqc --cores {threads} \
            --output_dir {OUTPUT}/trimmed \
            {input.r1} {input.r2} &> {log}
        """

# Rule to align trimmed reads
rule align:
    input:
        r1 = rules.trim_reads.output.r1,
        r2 = rules.trim_reads.output.r2
    output:
        bam = temp(join(OUTPUT, "aligned", "{sample}.unsorted.bam")),
        sorted_bam = join(OUTPUT, "aligned", "{sample}.bam"),
        bai = join(OUTPUT, "aligned", "{sample}.bam.bai")
    log:
        join(OUTPUT, "logs", "bowtie2", "{sample}.log")
    threads: 32
    resources:
        mem_mb=96000,
        time="8:00:00"
    params:
        max_fragment = config["bowtie2_params"]["max_fragment_length"],
        sort_memory = "2G",
        tmp_dir = "/beegfs/scratch/ric.broccoli/kubacki.michal/SRF_CUTandTAG/custom_pipeline/tmp"
    envmodules:
        "bowtie2/2.4.2",
        "samtools/1.13"
    shell:
        """
        # Create temporary directory for this job
        TEMP_DIR=$(mktemp -d -p {params.tmp_dir})
        
        # Align reads with optimized parameters
        bowtie2 \
            -p {threads} \
            -x {config[genome_index]} \
            -1 {input.r1} -2 {input.r2} \
            --local --very-sensitive-local \
            --no-mixed --no-discordant \
            --maxins {params.max_fragment} \
            --mm \
            2> {log} | \
        samtools view -@ {threads} -bS -q 30 - > {output.bam}

        # Sort BAM file using multiple threads and temporary directory
        samtools sort \
            -@ {threads} \
            -m {params.sort_memory} \
            -T $TEMP_DIR/{wildcards.sample} \
            -o {output.sorted_bam} \
            {output.bam}

        # Index BAM file
        samtools index -@ {threads} {output.sorted_bam}
        
        # Cleanup
        rm -rf $TEMP_DIR
        """

# Rule to call peaks using MACS2
rule call_peaks:
    input:
        treatment = join(OUTPUT, "aligned", "{sample}.bam"),
        treatment_index = join(OUTPUT, "aligned", "{sample}.bam.bai"),
        control = lambda wildcards: join(OUTPUT, "aligned", 
                  f"{CONTROL_SAMPLES[get_experiment(wildcards.sample)]}.bam"),
        control_index = lambda wildcards: join(OUTPUT, "aligned", 
                       f"{CONTROL_SAMPLES[get_experiment(wildcards.sample)]}.bam.bai")
    output:
        peaks = join(OUTPUT, "peaks", "{sample}_peaks.narrowPeak")
    log:
        join(OUTPUT, "logs", "macs2", "{sample}.log")
    threads: 16
    resources:
        mem_mb=32000,
        time="4:00:00"
    params:
        genome_size = config['effective_genome_size'],
        format = config['macs2_params']['format'],
        pvalue = config['macs2_params']['pvalue'],
        outdir = lambda wildcards, output: os.path.dirname(output.peaks)
    envmodules:
        "macs2/2.2.7.1"
    shell:
        """
        macs2 callpeak \
            -t {input.treatment} \
            -c {input.control} \
            -f {params.format} \
            -g {params.genome_size} \
            -n {wildcards.sample} \
            --outdir {params.outdir} \
            -p {params.pvalue} \
            --nomodel \
            --shift {config[macs2_params][shift]} \
            --extsize {config[macs2_params][extsize]} \
            --keep-dup {config[macs2_params][keep_dup]} \
            2> {log}
        """

# Rule to run MultiQC to aggregate QC reports
# compiles all quality control metrics into a single report
rule multiqc:
    # takes all fastqc and aligned bam files from previous rules
    input:
        expand(join(OUTPUT, "fastqc", "{sample}_R1_001_fastqc.html"), sample=ALL_SAMPLES),
        expand(join(OUTPUT, "fastqc", "{sample}_R2_001_fastqc.html"), sample=ALL_SAMPLES),
        expand(join(OUTPUT, "aligned", "{sample}.bam"), sample=ALL_SAMPLES)
    # outputs multiqc report
    output:
        report = join(OUTPUT, "multiqc", "multiqc_report.html")
    log:
        join(OUTPUT, "logs", "multiqc", "multiqc.log")
    shell:
        "multiqc {OUTPUT} -o {OUTPUT}/multiqc &> {log}"

# Rule to perform differential binding analysis using DiffBind
# identifies differentially bound regions between conditions
# rule differential_binding:
#     # takes peaks and aligned bam files from call_peaks and align rules
#     input:
#         peaks = lambda wildcards: expand(join(OUTPUT, "peaks", "{sample}_peaks.narrowPeak"), 
#                                          sample=get_samples_for_experiment(wildcards.experiment)),
#         bams = lambda wildcards: expand(join(OUTPUT, "aligned", "{sample}.bam"), 
#                                         sample=get_samples_for_experiment(wildcards.experiment))
#     # outputs csv file with differential binding results
#     output:
#         results = join(OUTPUT, "differential_binding", "{experiment}_differential_binding_results.csv")
#     log:
#         join(OUTPUT, "logs", "diffbind", "{experiment}_diffbind.log")
#     threads: 32
#     conda:
#         "r_env"
#     script:
#         "scripts/run_diffbind.R"

# Helper function to determine the input directory for a given sample
def get_input_dir(sample):
    # checks which experiment the sample belongs to and returns the corresponding input directory
    if sample in EXOGENOUS_SAMPLES:
        return "DATA/EXOGENOUS"
    elif sample in ENDOGENOUS_SAMPLES:
        return "DATA/ENDOGENOUS"
    elif sample in POL2_SAMPLES:
        return "DATA/POL2"
    else:
        raise ValueError(f"Unknown sample: {sample}")

# Helper function to determine the experiment type for a given sample
def get_experiment(sample):
    # checks which experiment the sample belongs to and returns the corresponding experiment name
    if sample in EXOGENOUS_SAMPLES:
        return "EXOGENOUS"
    elif sample in ENDOGENOUS_SAMPLES:
        return "ENDOGENOUS"
    elif sample in POL2_SAMPLES:
        return "POL2"
    else:
        raise ValueError(f"Unknown sample: {sample}")

# Helper function to get samples for a specific experiment type
def get_samples_for_experiment(experiment):
    # checks which experiment the sample belongs to and returns the corresponding experiment name
    if experiment == "EXOGENOUS":
        return EXOGENOUS_SAMPLES
    elif experiment == "ENDOGENOUS":
        return ENDOGENOUS_SAMPLES
    elif experiment == "POL2":
        return POL2_SAMPLES
    else:
        raise ValueError(f"Unknown experiment: {experiment}")

# Add localrules for non-computational steps
localrules: all, multiqc

# Add rule to analyze fragment sizes
rule fragment_sizes:
    input:
        bam = join(OUTPUT, "aligned", "{sample}.bam"),
        bai = join(OUTPUT, "aligned", "{sample}.bam.bai")
    output:
        pdf = join(OUTPUT, "fragment_sizes", "{sample}_fragment_sizes.pdf"),
        txt = join(OUTPUT, "fragment_sizes", "{sample}_fragment_sizes.txt")
    log:
        join(OUTPUT, "logs", "fragment_sizes", "{sample}.log")
    threads: 16
    resources:
        mem_mb=16000,
        time="1:00:00"
    envmodules:
        "samtools/1.13",
        "R/4.1.0"
    shell:
        """
        # Calculate fragment sizes
        samtools view {input.bam} | \
        awk '{{if ($9 > 0) print $9}}' > {output.txt}

        # Plot fragment size distribution
        Rscript -e '
        pdf("{output.pdf}")
        data <- read.table("{output.txt}")
        hist(data$V1, breaks=100, main="{wildcards.sample} Fragment Size Distribution",
             xlab="Fragment Size", ylab="Count")
        dev.off()
        ' 2> {log}
        """

# Rule to perform extended analysis
rule extended_analysis:
    input:
        peaks = expand(join(OUTPUT, "peaks", "{sample}_peaks.narrowPeak"), sample=ALL_SAMPLES),
        bams = expand(join(OUTPUT, "aligned", "{sample}.bam"), sample=ALL_SAMPLES)
    output:
        report = join(OUTPUT, "extended_analysis", "analysis_report.html")
    log:
        join(OUTPUT, "logs", "extended_analysis", "extended_analysis.log")
    script:
        "scripts/extended_analysis.R"

rule cross_correlation:
    input:
        bam = rules.align.output.sorted_bam
    output:
        plot = join(OUTPUT, "qc", "{sample}.crosscorr.pdf"),
        metrics = join(OUTPUT, "qc", "{sample}.crosscorr.txt")
    script:
        "scripts/cross_correlation.R"

rule calculate_frip:
    input:
        bam = rules.align.output.sorted_bam,
        peaks = rules.call_peaks.output.peaks
    output:
        frip = join(OUTPUT, "qc", "{sample}.frip_score.txt")
    script:
        "scripts/calculate_frip.R"

rule generate_md5:
    input:
        fastq = lambda wildcards: join(get_input_dir(wildcards.sample), 
                                     f"{wildcards.sample}_R{wildcards.read}_001.fastq.gz")
    output:
        md5 = join(OUTPUT, "md5", "{sample}_R{read}_001.md5")
    shell:
        "md5sum {input.fastq} > {output.md5}"

rule generate_report:
    input:
        qc_metrics = expand(join(OUTPUT, "qc", "{sample}.{metric}.txt"),
                          sample=ALL_SAMPLES,
                          metric=["frip_score", "tss_enrichment"]),
        peak_metrics = join(OUTPUT, "peak_analysis", "peak_reproducibility.txt")
    output:
        report = join(OUTPUT, "report", "pipeline_report.html")
    script:
        "scripts/generate_report.R"

# Add rule for bigwig generation
rule make_bigwig:
    input:
        bam = join(OUTPUT, "aligned", "{sample}.bam"),
        bai = join(OUTPUT, "aligned", "{sample}.bam.bai")
    output:
        bw = join(OUTPUT, "bigwig", "{sample}.bw")
    params:
        bin_size = config["visualization"]["bigwig"]["bin_size"],
        smooth_length = config["visualization"]["bigwig"]["smooth_length"]
    log:
        join(OUTPUT, "logs", "bigwig", "{sample}.log")
    threads: 4
    envmodules:
        "deeptools/3.5.1"
    shell:
        """
        bamCoverage --bam {input.bam} \
            --outFileName {output.bw} \
            --binSize {params.bin_size} \
            --smoothLength {params.smooth_length} \
            --normalizeUsing RPKM \
            --numberOfProcessors {threads} \
            2> {log}
        """

# Add rule for TSS enrichment
rule tss_enrichment:
    input:
        bam = join(OUTPUT, "aligned", "{sample}.bam"),
        bai = join(OUTPUT, "aligned", "{sample}.bam.bai")
    output:
        score = join(OUTPUT, "qc", "{sample}.tss_enrichment.txt")
    params:
        gtf = config["tss_analysis"]["genome_gtf"],
        upstream = config["tss_analysis"]["upstream"],
        downstream = config["tss_analysis"]["downstream"]
    log:
        join(OUTPUT, "logs", "tss_enrichment", "{sample}.log")
    envmodules:
        "R/4.1.0"
    script:
        "scripts/calculate_tss_enrichment.R"

# Add rule for metagene plot
rule metagene_plot:
    input:
        bw = join(OUTPUT, "bigwig", "{sample}.bw"),
        peaks = join(OUTPUT, "peaks", "{sample}_peaks.narrowPeak")
    output:
        plot = join(OUTPUT, "plots", "{sample}_metagene.pdf")
    params:
        upstream = config["tss_analysis"]["upstream"],
        downstream = config["tss_analysis"]["downstream"]
    log:
        join(OUTPUT, "logs", "plots", "{sample}_metagene.log")
    envmodules:
        "R/4.1.0"
    script:
        "scripts/plot_metagene.R"

# Add rule for heatmap
rule heatmap_plot:
    input:
        bw = join(OUTPUT, "bigwig", "{sample}.bw"),
        peaks = join(OUTPUT, "peaks", "{sample}_peaks.narrowPeak")
    output:
        plot = join(OUTPUT, "plots", "{sample}_heatmap.pdf")
    params:
        window_size = config["visualization"]["heatmap"]["window_size"],
        bin_size = config["visualization"]["heatmap"]["bin_size"]
    log:
        join(OUTPUT, "logs", "plots", "{sample}_heatmap.log")
    envmodules:
        "R/4.1.0"
    script:
        "scripts/plot_heatmap.R"

# Rule to analyze peak reproducibility
rule analyze_peak_reproducibility:
    input:
        peaks = expand(join(OUTPUT, "peaks", "{sample}_peaks.narrowPeak"), sample=ALL_SAMPLES)
    output:
        txt = join(OUTPUT, "peak_analysis", "peak_reproducibility.txt")
    log:
        join(OUTPUT, "logs", "peak_analysis", "reproducibility.log")
    threads: 4
    resources:
        mem_mb=8000,
        time="1:00:00"
    envmodules:
        "R/4.1.0"
    shell:
        """
        Rscript scripts/analyze_reproducibility.R \
            --peaks {input.peaks} \
            --output {output.txt} \
            2> {log}
        """

# Rule to annotate peaks
rule annotate_peaks:
    input:
        peaks = expand(join(OUTPUT, "peaks", "{sample}_peaks.narrowPeak"), sample=ALL_SAMPLES)
    output:
        txt = join(OUTPUT, "peak_analysis", "peak_annotation.txt")
    log:
        join(OUTPUT, "logs", "peak_analysis", "annotation.log")
    threads: 4
    resources:
        mem_mb=8000,
        time="1:00:00"
    envmodules:
        "R/4.1.0"
    shell:
        """
        Rscript scripts/annotate_peaks.R \
            --peaks {input.peaks} \
            --output {output.txt} \
            --genome {config[genome_gtf]} \
            2> {log}
        """

